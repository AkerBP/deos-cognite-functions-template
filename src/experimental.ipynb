{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcognite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_classes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSchedulesList\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcognite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_classes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSchedule\n\u001b[1;32m---> 11\u001b[0m parent_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m)))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parent_path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mpath:\n\u001b[0;32m     13\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(parent_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from cognite.client.data_classes import functions\n",
    "from cognite.client.data_classes.functions import FunctionSchedulesList\n",
    "from cognite.client.data_classes.functions import FunctionSchedule\n",
    "\n",
    "from initialize import initialize_client\n",
    "cdf_env = \"dev\"\n",
    "\n",
    "# Set limit on function calls - don't think it's really necessary ...\n",
    "func_limits = functions.FunctionsLimits(timeout_minutes=60, cpu_cores=0.25, memory_gb=1, runtimes=[\"py39\"], response_size_mb=2)\n",
    "client = initialize_client(cdf_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://pi-web-api.akerbp.com/piwebapi/streams/F1DPGrFe7LeAxUu_IwydrHHT2wNzEIAAQVBQLVBJLVNFUlZFUlwwMiBBTU9TLkVBU1QgS0FNRUxFT04gRkxPVyBBTEFSTS5DMEZGNUVFNy1CQUZBLTQzRDMtQTQ5Qy02RTQ5Njg5QUVCQTY/recorded?startTime=*-7d&maxCount=2\"\n",
    "auth = (\"vetnev\", \"test\")\n",
    "r = requests.get(url, auth=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Need to store model parameters in file, and upload this to CDF to be able to extract trained parameters for next call to the Cognite Function (fine-tuning for next schedule and prediction for subsequent period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579500946117362"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data[col]: prints pd.Series object\n",
    "# data[[col]]: prints pd.DataFrame object\n",
    "\n",
    "myfunc = client.functions.retrieve(external_id=\"cf_power-Template\")\n",
    "my_schedule_id = client.functions.schedules.list(\n",
    "                name=\"VAL_11-LT-95107A:X.Value\", function_id=myfunc.id).to_pandas().id[0]\n",
    "\n",
    "scheduled_calls = myfunc.list_calls(schedule_id=my_schedule_id, limit=-1).to_pandas()\n",
    "start_time = (datetime(2024, 1, 15, 15, 30) - datetime(1970, 1, 1, 0, 0)).total_seconds()*1000\n",
    "end_time = (datetime(2024, 1, 15, 15, 45) - datetime(1970, 1, 1, 0, 0)).total_seconds()*1000\n",
    "\n",
    "mask_start = scheduled_calls[\"scheduled_time\"] >= start_time\n",
    "mask_end = scheduled_calls[\"scheduled_time\"] < end_time\n",
    "myid = scheduled_calls[mask_start & mask_end][\"id\"].iloc[0]\n",
    "myfunc.retrieve_call(id=myid).get_response()\n",
    "myfunc.list_calls().to_pandas().id[0]\n",
    "# test = client.functions.calls.retrieve(call_id=3005253751851002, function_id=84587311037983).get_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series future prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vetnev\\AppData\\Local\\anaconda3\\envs\\task1\\Lib\\site-packages\\cognite\\client\\_api\\datapoints.py:806: UserWarning: Your installation of 'protobuf' is missing compiled C binaries, and will run in pure-python mode, which causes datapoints fetching to be ~5x slower. To verify, set the environment variable `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp` before running (this will cause the code to fail). The easiest fix is probably to pin your 'protobuf' dependency to major version 4 (or higher), see: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
      "  fetcher = select_dps_fetch_strategy(self, user_query=query)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VAL_11-LT-95107A:X.Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:00</th>\n",
       "      <td>65.737658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:10:00</th>\n",
       "      <td>65.781379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:20:00</th>\n",
       "      <td>65.729704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:30:00</th>\n",
       "      <td>65.749212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:40:00</th>\n",
       "      <td>65.773900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     VAL_11-LT-95107A:X.Value\n",
       "2023-01-01 00:00:00                 65.737658\n",
       "2023-01-01 00:10:00                 65.781379\n",
       "2023-01-01 00:20:00                 65.729704\n",
       "2023-01-01 00:30:00                 65.749212\n",
       "2023-01-01 00:40:00                 65.773900"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_input_name = \"VAL_11-LT-95107A:X.Value\"\n",
    "ts_list = client.time_series.list(name=ts_input_name).to_pandas()\n",
    "ts_extid = ts_list[\"external_id\"][0]\n",
    "\n",
    "ts_95107A = client.time_series.data.retrieve(external_id=ts_extid,\n",
    "                                             granularity=\"10m\",\n",
    "                                             aggregates=\"average\",\n",
    "                                             start=pd.to_datetime(datetime(2023,1,1)),\n",
    "                                             end=pd.to_datetime(pd.Timestamp.now())).to_pandas()\n",
    "\n",
    "ts_95107A = ts_95107A.rename(columns={ts_extid+\"|average\": ts_input_name})\n",
    "ts_95107A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test splits\n",
    "max_idx = len(ts_95107A)-1\n",
    "train_test_split = 0.7\n",
    "X_train = ts_95107A.iloc[:int(train_test_split*max_idx)] # for CV grid search\n",
    "X_test = ts_95107A.iloc[int(train_test_split*max_idx):] # to test for best model found from CV grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "test_size = 100 # here: 100 10-min periods\n",
    "\n",
    "# 1. Use expanding window feature from Anders, or manually divide train set into expanding windows\n",
    "# 2. If manual, make an expanding window model Class so to give a valid model input to CV GridSearch\n",
    "# 3. Run CV GridSearch, extract best model\n",
    "# 4. use best model to predict data in test set (most recent period OR future period). If future period, change train set to cover all data until current date, then define test set to be from last datapoint and forward 100 10-min periods\n",
    "# 5. Plot historic and future data, but highlight future data in another color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandingWindowCV(BaseCrossValidator):\n",
    "    def __init__(self, n_splits, start_len, test_len):\n",
    "        self.n_splits = n_splits\n",
    "        self.start_len = start_len\n",
    "        self.test_len = test_len\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        N = len(X)\n",
    "        ws = int(np.floor(N / self.n_splits) - 1)\n",
    "        for i in range(self.n_splits):\n",
    "            Xi_train = X[:self.start_len + i*ws]\n",
    "            Xi_test = X[self.start_len+i*ws:self.start_len+i*ws+self.test_len]\n",
    "            yield Xi_train, Xi_test\n",
    "\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ExpandingWindowCV(n_splits=5, start_len=500, test_len=100)\n",
    "# a = exp.split(ts_95107A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Pipeline([\n",
    "                (\"poly\", PolynomialFeatures(degree=2, interaction_only=True)),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"regressor\", BayesianRidge(fit_intercept=True))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    \"lambda_1\": [1e-6, 1e-5],\n",
    "    \"alpha_1\": [1e-6, 1e-5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of fits: 20\n"
     ]
    }
   ],
   "source": [
    "model = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    cv=cv,\n",
    "    param_grid=parameter_grid,\n",
    "    n_jobs=2,\n",
    "    scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
    "    return_train_score=True,\n",
    "    refit=True,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "print(\"Total number of fits:\", cv.get_n_splits(X_train) * len(list(itertools.product(*parameter_grid.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = pathlib.Path(\"\").absolute().joinpath(\"../data/experimental\")\n",
    "if not exp_dir.exists():\n",
    "    exp_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1 - Train from scratch.\", flush=True)\n",
    "print(\"2 - Load results from previous training.\", flush=True)\n",
    "choice = input(\"What do you want to do?\")\n",
    "\n",
    "if int(choice) == 1:\n",
    "    model.fit(X_train, y_train)\n",
    "    cv_results = pd.DataFrame(model.cv_results_).sort_values(by=\"rank_test_score\", ascending=True)\n",
    "    cv_results.to_csv(exp_dir.joinpath(\"cross_validation_results.csv\"))\n",
    "    with open(exp_dir.joinpath(\"best_model.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(obj=model.best_estimator_, file=f)\n",
    "    print(f\">>> ({choice}) Model trained from scratch and cross validation results written to file.\")\n",
    "elif int(choice) == 2:\n",
    "    cv_results = pd.read_csv(exp_dir.joinpath(\"cross_validation_results.csv\"), index_col=0).sort_values(by=\"rank_test_score\", ascending=True)\n",
    "    print(f\">>> ({choice}) Loaded cross validation results from file.\")\n",
    "else:\n",
    "    raise RuntimeError(\"Invalid choice\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firsttask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
