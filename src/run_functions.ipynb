{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for deploying Cognite Function\n",
    "\n",
    "Run all cells sequentially until `Experimental` section to deploy your Cognite Function.\n",
    "\n",
    "Modifications are done in `Inputs` section, where you need to supply relevant input parameters as required by instantiation, calculations and deployment of your Cognite Function. The input parameters related to calculations and deployment are stored in `data_dict`. There are two types of input parameters:\n",
    "- A: General parameters required for deployment of any Cognite Function\n",
    "- B: Optional parameters for more detailed specifications\n",
    "- C: Calculation-specific parameters relevant for your calculations defined in `transformation.py` in the associated Cognite Functions subfolder\n",
    "\n",
    "If your Cognite Function is already instantiated, but you want to set up a new schedule, you can omit calling `generate_cf` and skip straight to calling `deploy_cognite_functions` with a modified `data_dict` of parameters that satisfy your scheduled calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Authentication ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from cognite.client.data_classes import functions\n",
    "\n",
    "\n",
    "from initialize_cdf_client import initialize_cdf_client\n",
    "from deploy_cognite_functions import deploy_cognite_functions\n",
    "from generate_cf import generate_cf\n",
    "from utilities import dataset_abbreviation\n",
    "\n",
    "cdf_env = \"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set limit on function calls - don't think it's really necessary ...\n",
    "func_limits = functions.FunctionsLimits(timeout_minutes=60, cpu_cores=0.25, memory_gb=1, runtimes=[\"py39\"], response_size_mb=2)\n",
    "client = initialize_cdf_client(cdf_env)\n",
    "main_desc = \" Produced with Cognite Functions Template.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.time_series.delete(external_id=\"CoEA_WastedEnergy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Inputs ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Required parameters\n",
    "- `ts_input_names` (list): \n",
    "    - names of input time series (a list, even if only one input). Must be given in same order as calculations are performed in `transformations.py`\n",
    "- `ts_output` (dict): \n",
    "    - metadata for output time series, currently supporting the following fields:\n",
    "    1. `names` (list of strings):\n",
    "        - names of time series output(s)\n",
    "        - NB: if multiple time series outputs, order of ts_output_names must correspond to order in ts_input_names.\n",
    "    2. `description` (list of strings):\n",
    "        - description for each time series output\n",
    "    3. `unit` (list of strings):\n",
    "        - units used for each time series output\n",
    "- `dataset_id` (int):\n",
    "    - id of dataset to write data to\n",
    "- `function_name` (string): \n",
    "    - name of Cognite Function to deploy, instantiating a folder `*dataset_abbr*_*function_name*` where *dataset_abbr* is and abbreviation of the name of the dataset to write to (see optional parameters in next section) \n",
    "    - for example: `function_name=\"wasted-energy\"` for a Cognite Function that is to calculate wasted energy\n",
    "- `calculation_function` (string): \n",
    "    - name of main calculation function to run, should be defined in transformation.py (in the folder `*dataset_name*_*function_name*`) as `main_*calculation_function*`\n",
    "    - for example: `calculation_function=\"wasted_energy\"` to use a function `main_wasted_energy` defined in `transformation.py` of the  `*dataset_name*_wasted-energy` folder\n",
    "- `schedule_name` (string):\n",
    "    - name of schedule to set up for the Cognite Function. NB: make sure name is unique to avoid overwriting already existing schedules for a particular Cognite Function! If setting up multiple schedules for the same Cognite Function, one for each input time series, a good advice to keep them organized is to use the name of the time series as the name of the schedules\n",
    "- `sampling_rate` (string): \n",
    "    - sampling rate of input time series\n",
    "    - given as value followed by time unit, e.g., \"30s\" for 30 seconds, \"2m\" for 2 minutes, \"1h\" for 1 hour, etc ...\n",
    "- `cron_interval_min` (string): \n",
    "    - minute-interval to run schedule at (NB: currently only supported for min-interval [1, 60)). The number should be provided as string.\n",
    "- `backfill_period` (int): \n",
    "    - the period (default: number of days) back in time to perform backfilling\n",
    "- `backfill_hour` (int):\n",
    "    - the hour of the day to perform backfilling\n",
    "- `backfill_min_start` (int):\n",
    "    - performs backfilling for any scheduled call that falls within hour=`backfill_hour` and minute=`[backfill_min_start, backfill_min_start+cron_interval_min]`\n",
    "- `add_packages` (list): \n",
    "    - additional packages required to run the calculations defined in `transformations.py` from the Cognite Function subfolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CF 1: Ideal Power Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_input_names = [\"VAL_17-FI-9101-286:VALUE\", \"VAL_17-PI-95709-258:VALUE\", \"VAL_11-PT-92363B:X.Value\", \"VAL_11-XT-95067B:Z.X.Value\"] # Inputs to IdealPowerConsumption function # [\"VAL_11-XT-95067B:Z.X.Value\", 87.8, \"CF_IdealPowerConsumption\"] # Inputs to WastedEnergy function\n",
    "# ts_input_names = [\"VAL_11-LT-95107A:X.Value\"]\n",
    "ts_output = {\"names\": [\"CoEA_IdealPowerConsumption\"],\n",
    "             \"description\": [\"Optimal power consumption from equipment.\" + main_desc], #[\"Daily average drainage from pump\"]\n",
    "             \"unit\": [\"J/s\"]} #[\"m3/min\"]\n",
    "dataset_id = 1832663593546318\n",
    "\n",
    "function_name = \"ideal-power-consumption\"\n",
    "calculation_function = \"ideal_power_consumption\"\n",
    "schedule_name = \"ipc\"#ts_input_names[0]\n",
    "\n",
    "sampling_rate = \"1m\"\n",
    "cron_interval_min = str(15) #\n",
    "assert int(cron_interval_min) < 60 and int(cron_interval_min) >= 1\n",
    "\n",
    "backfill_period = 20\n",
    "backfill_hour = 11 # 23\n",
    "backfill_min_start = 0\n",
    "backfill_min_start = min(59, backfill_min_start)\n",
    "\n",
    "add_packages = []#[\"statsmodels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CF 2: Wasted energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_input_names = [\"VAL_11-XT-95067B:Z.X.Value\", 87.8, \"CoEA_IdealPowerConsumption\"] # Inputs to IdealPowerConsumption function # [\"VAL_11-XT-95067B:Z.X.Value\", 87.8, \"CF_IdealPowerConsumption\"] # Inputs to WastedEnergy function\n",
    "# ts_input_names = [\"VAL_11-LT-95107A:X.Value\"]\n",
    "ts_output = {\"names\": [\"CoEA_WastedEnergy\"],\n",
    "             \"description\": [\"Wasted energy from equipment, calculated from ideal power consumption.\" + main_desc], #[\"Daily average drainage from pump\"]\n",
    "             \"unit\": [\"J/s\"]} #[\"m3/min\"]\n",
    "dataset_id = 1832663593546318\n",
    "\n",
    "function_name = \"wasted-energy\"\n",
    "calculation_function = \"wasted_energy\"\n",
    "schedule_name = \"we\"#ts_input_names[0]\n",
    "\n",
    "sampling_rate = \"1m\"\n",
    "cron_interval_min = str(15) #\n",
    "assert int(cron_interval_min) < 60 and int(cron_interval_min) >= 1\n",
    "\n",
    "backfill_period = 20\n",
    "backfill_hour = 19 # 23\n",
    "backfill_min_start = 0\n",
    "backfill_min_start = min(59, backfill_min_start)\n",
    "\n",
    "add_packages = []#[\"statsmodels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CF 3: Daily average drainage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_input_names = [\"VAL_17-FI-9101-286:VALUE\", \"VAL_17-PI-95709-258:VALUE\", \"VAL_11-PT-92363B:X.Value\", \"VAL_11-XT-95067B:Z.X.Value\"] # Inputs to IdealPowerConsumption function # [\"VAL_11-XT-95067B:Z.X.Value\", 87.8, \"CF_IdealPowerConsumption\"] # Inputs to WastedEnergy function\n",
    "ts_input_names = [\"VAL_18-LIT-80143:VALUE\"]\n",
    "ts_output = {\"names\": [\"VAL_18-LIT-80143.CDF.D.AVG.LeakValue\"],\n",
    "             \"description\": [\"Daily average drainage from pump.\"+main_desc],\n",
    "             \"unit\": [\"m3/min\"]}\n",
    "dataset_id = 1832663593546318\n",
    "\n",
    "function_name = \"avg-leakage\"\n",
    "calculation_function = \"aggregate\"\n",
    "schedule_name = ts_input_names[0]\n",
    "\n",
    "sampling_rate = \"1m\" #\n",
    "cron_interval_min = str(15) #\n",
    "assert int(cron_interval_min) < 60 and int(cron_interval_min) >= 1\n",
    "\n",
    "backfill_period = 3\n",
    "backfill_hour = 15 # 23\n",
    "backfill_min_start = 0\n",
    "backfill_min_start = min(59, backfill_min_start)\n",
    "\n",
    "add_packages = [\"statsmodels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Optional parameters (if no, leave empty)\n",
    "- `historic_start_time` (dictionary):\n",
    "    - date to start from when performing initial calculation of signal\n",
    "    - recommended if millions of data points in full historic signal\n",
    "    - three keys must be specified\n",
    "    1. `year`\n",
    "    2. `month`\n",
    "    3. `day`\n",
    "- `aggregate` (dictionary):\n",
    "    - information about any aggregations to perform in the calculation\n",
    "    - if performing aggregates, two keys **must** be specified:\n",
    "    1. `period` (string):\n",
    "        - the time range defining the aggregated period\n",
    "        - valid values: `[\"minute\", \"hour\", \"day\", \"month\", \"year\"]`\n",
    "    2. `type` (string):\n",
    "        - what type of aggregate to perform\n",
    "        - valid values: any aggregation supported by `pandas`, e.g., `\"mean\"`, `\"max\"`, ...\n",
    "- `dataset_abbr` (string):\n",
    "    - abbreviated name for dataset, used as prefix for Cognite Function for better structure\n",
    "    - if not provided, an abbreviated form will be automatically generated from the name of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optional = {\n",
    "    \"historic_start_time\": {\n",
    "        \"year\": 2022,\n",
    "        \"month\": 10,\n",
    "        \"day\": 1\n",
    "    },\n",
    "    \"aggregate\": {\n",
    "        \"period\": \"day\",\n",
    "        \"type\": \"mean\"\n",
    "    }\n",
    "    # \"dataset_abbr\": \"PIts\" # PI Time Series\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Calculation-specific parameters (if no, leave empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_params = {\n",
    "    \"tank_volume\": 240,\n",
    "    \"derivative_value_excl\": 0.002,\n",
    "    \"lowess_frac\": 0.001,\n",
    "    \"lowess_delta\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert parameters into data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_abbr = dataset_abbreviation(client, optional, dataset_id)\n",
    "\n",
    "data_dict = {'ts_input_names':ts_input_names,\n",
    "            'ts_output':ts_output,\n",
    "            'function_name': f\"{dataset_abbr}_{function_name}\",\n",
    "            'schedule_name': schedule_name,\n",
    "            'calculation_function': f\"main_{calculation_function}\",\n",
    "            'granularity': sampling_rate,\n",
    "            'dataset_id': dataset_id, # Center of Excellence - Analytics dataset\n",
    "            'cron_interval_min': cron_interval_min,\n",
    "            'testing': False,\n",
    "            'backfill_period': backfill_period, # days by default (if not doing aggregates)\n",
    "            'backfill_hour': backfill_hour, # 23: backfilling to be scheduled at last hour of day as default\n",
    "            'backfill_min_start': backfill_min_start, 'backfill_min_end': min(59.9, backfill_min_start + int(cron_interval_min)),\n",
    "            'optional': optional,\n",
    "            'calc_params': calc_params\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Instantiate Cognite Function ---\n",
    "\n",
    "Set up folder structure for the Cognite Function as required by the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing __init__.py ...\n",
      "Writing handler.py ...\n",
      "Writing transformation.py ...\n",
      "Created requirements.txt in c:/Users/vetnev/OneDrive - Aker BP/Documents/First Task/opshub-task1/src/CoEA_avg-leakage\n",
      "Packages to add:  ['pytest', 'pyarrow', 'statsmodels', 'python-dotenv', 'ipykernel', 'pandas', 'numpy', 'cognite-sdk', 'openpyxl']\n",
      "\n",
      "Using version ^7.4.4 for pytest\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 5 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing colorama (0.4.6)\n",
      "  â€¢ Installing iniconfig (2.0.0)\n",
      "  â€¢ Installing packaging (23.2)\n",
      "  â€¢ Installing pluggy (1.4.0)\n",
      "  â€¢ Installing pytest (7.4.4)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^15.0.0 for pyarrow\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 2 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing numpy (1.26.3)\n",
      "  â€¢ Installing pyarrow (15.0.0)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^0.14.1 for statsmodels\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 8 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing six (1.16.0)\n",
      "  â€¢ Installing python-dateutil (2.8.2)\n",
      "  â€¢ Installing pytz (2023.3.post1)\n",
      "  â€¢ Installing tzdata (2023.4)\n",
      "  â€¢ Installing pandas (2.2.0)\n",
      "  â€¢ Installing patsy (0.5.6)\n",
      "  â€¢ Installing scipy (1.12.0)\n",
      "  â€¢ Installing statsmodels (0.14.1)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^1.0.1 for python-dotenv\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 1 install, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing python-dotenv (1.0.1)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^6.29.0 for ipykernel\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 24 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing asttokens (2.4.1)\n",
      "  â€¢ Installing executing (2.0.1)\n",
      "  â€¢ Installing parso (0.8.3)\n",
      "  â€¢ Installing platformdirs (4.1.0)\n",
      "  â€¢ Installing pure-eval (0.2.2)\n",
      "  â€¢ Installing pywin32 (306)\n",
      "  â€¢ Installing traitlets (5.14.1)\n",
      "  â€¢ Installing wcwidth (0.2.13)\n",
      "  â€¢ Installing decorator (5.1.1)\n",
      "  â€¢ Installing jedi (0.19.1)\n",
      "  â€¢ Installing jupyter-core (5.7.1)\n",
      "  â€¢ Installing matplotlib-inline (0.1.6)\n",
      "  â€¢ Installing prompt-toolkit (3.0.43)\n",
      "  â€¢ Installing pygments (2.17.2)\n",
      "  â€¢ Installing pyzmq (25.1.2)\n",
      "  â€¢ Installing stack-data (0.6.3)\n",
      "  â€¢ Installing tornado (6.4)\n",
      "  â€¢ Installing comm (0.2.1)\n",
      "  â€¢ Installing debugpy (1.8.0)\n",
      "  â€¢ Installing ipython (8.20.0)\n",
      "  â€¢ Installing jupyter-client (8.6.0)\n",
      "  â€¢ Installing nest-asyncio (1.6.0)\n",
      "  â€¢ Installing psutil (5.9.8)\n",
      "  â€¢ Installing ipykernel (6.29.0)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^2.2.0 for pandas\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^1.26.3 for numpy\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^7.15.1 for cognite-sdk\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 15 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing pycparser (2.21)\n",
      "  â€¢ Installing cffi (1.16.0)\n",
      "  â€¢ Installing certifi (2023.11.17)\n",
      "  â€¢ Installing charset-normalizer (3.3.2)\n",
      "  â€¢ Installing cryptography (42.0.1)\n",
      "  â€¢ Installing idna (3.6)\n",
      "  â€¢ Installing urllib3 (2.1.0)\n",
      "  â€¢ Installing oauthlib (3.2.2)\n",
      "  â€¢ Installing pyjwt (2.8.0)\n",
      "  â€¢ Installing requests (2.31.0)\n",
      "  â€¢ Installing msal (1.26.0)\n",
      "  â€¢ Installing protobuf (4.25.2)\n",
      "  â€¢ Installing requests-oauthlib (1.3.1)\n",
      "  â€¢ Installing typing-extensions (4.9.0)\n",
      "  â€¢ Installing cognite-sdk (7.15.1)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^3.1.2 for openpyxl\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 2 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing et-xmlfile (1.1.0)\n",
      "  â€¢ Installing openpyxl (3.1.2)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Installing dependencies from lock file\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "Created requirements.txt in c:/Users/vetnev/OneDrive - Aker BP/Documents/First Task/opshub-task1/src/CoEA_avg-leakage\n"
     ]
    }
   ],
   "source": [
    "generate_cf(data_dict, add_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Define transformation function ---\n",
    "\n",
    "**IMPORTANT**: Include your desired calculations in `transformation.py` before moving on to next step (deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Deploy Cognite Function in one go ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduled call\n",
    "\n",
    "Set up schedule of Cognite Function. \n",
    "\n",
    "\n",
    "**NB**: Be aware that initial transformation can be data intensive, and may require running locally before deploying schedule, or limit the start time of historic calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cognite Function created. Waiting for deployment status to be ready ...\n",
      "Ready for deployement.\n",
      "Preparing schedule to start sharp at next minute ...\n",
      "Setting up Cognite Function schedule at time 2024-01-26 16:38:00+00:00 ...\n",
      "... Done\n"
     ]
    }
   ],
   "source": [
    "deploy_cognite_functions(data_dict, client,\n",
    "                         single_call=False, scheduled_call=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Single call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cognite Function created. Waiting for deployment status to be ready ...\n",
      "Ready for deployement.\n",
      "Calling Cognite Function individually ...\n",
      "... Done\n"
     ]
    }
   ],
   "source": [
    "deploy_cognite_functions(data_dict, client,\n",
    "                         single_call=True, scheduled_call=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firsttask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
