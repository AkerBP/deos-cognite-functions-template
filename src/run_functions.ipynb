{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for deploying Cognite Function\n",
    "\n",
    "Run all cells sequentially until `Experimental` section to deploy your Cognite Function.\n",
    "\n",
    "Modifications are done in `Inputs` section, where you need to supply relevant input parameters as required by instantiation, calculations and deployment of your Cognite Function. The input parameters related to calculations and deployment are stored in `data_dict`. There are two types of input parameters:\n",
    "- A: General parameters required for deployment of any Cognite Function\n",
    "- B: Optional (calculation-specific) parameters used as input to your calculation function. These should enter `data_dict[\"calc_params\"]` as key-value pairs.\n",
    "\n",
    "If your Cognite Function is already instantiated, but you want to set up a new schedule, you can omit calling `generate_cf` and skip straight to calling `deploy_cognite_functions` with a modified `data_dict` of parameters that satisfy your scheduled calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Authentication ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from cognite.client.data_classes import functions\n",
    "from cognite.client.data_classes.functions import FunctionSchedulesList\n",
    "from cognite.client.data_classes.functions import FunctionSchedule\n",
    "\n",
    "from initialize import initialize_client\n",
    "from deploy_cognite_functions import deploy_cognite_functions\n",
    "from generate_cf import generate_cf\n",
    "\n",
    "cdf_env = \"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set limit on function calls - don't think it's really necessary ...\n",
    "func_limits = functions.FunctionsLimits(timeout_minutes=60, cpu_cores=0.25, memory_gb=1, runtimes=[\"py39\"], response_size_mb=2)\n",
    "client = initialize_client(cdf_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.time_series.delete(external_id=\"VAL_17-FI-9101-286:VALUE.COPY\")\n",
    "# client.time_series.delete(external_id=\"hourly_avg_drainage_description_unit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m----> 3\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_series\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexternal_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpi:156799\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[43maggregates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maverage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mgranularity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m60s\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vetnev\\OneDrive - Aker BP\\Documents\\First Task\\opshub-task1\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5365\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[0;32m   5363\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[0;32m   5364\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key)\n\u001b[1;32m-> 5365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   5368\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[0;32m   5369\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[0;32m   5370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n",
      "File \u001b[1;32mc:\\Users\\vetnev\\OneDrive - Aker BP\\Documents\\First Task\\opshub-task1\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:376\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03mThis getitem defers to the underlying array, which by-definition can\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03monly handle list-likes, slices, and integer scalars\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Use cast as we know we will get back a DatetimeLikeArray or DTScalar,\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# but skip evaluating the Union at runtime for performance\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# (see https://github.com/pandas-dev/pandas/pull/44624)\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m result \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnion[Self, DTScalarOrNaT]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_scalar(result):\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\vetnev\\OneDrive - Aker BP\\Documents\\First Task\\opshub-task1\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:276\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    272\u001b[0m     key: PositionalIndexer2D,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self \u001b[38;5;241m|\u001b[39m Any:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_integer(key):\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# fast-path\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ndarray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_box_func(result)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "client.time_series.data.retrieve(external_id=\"pi:156799\",\n",
    "                                aggregates=\"average\",\n",
    "                                granularity=\"60s\",\n",
    "                                start=pd.to_datetime(datetime(2024,1,17)),\n",
    "                                end=pd.to_datetime(datetime(2024,1,18)),\n",
    "                                limit=None).to_pandas().index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Inputs ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Required parameters\n",
    "- `ts_input_names` (list): \n",
    "    - names of input time series (a list, even if only one input). Must be given in same order as calculations are performed in `transformations.py`\n",
    "- `ts_output_names` (dict): \n",
    "    - metadata for output time series, currently supporting the following fields:\n",
    "    1. `names` (list of strings):\n",
    "        - names of time series output(s)\n",
    "        - NB: if multiple time series outputs, order of ts_output_names must correspond to order in ts_input_names.\n",
    "    2. `description` (list of strings):\n",
    "        - description for each time series output\n",
    "    3. `unit` (list of strings):\n",
    "        - units used for each time series output\n",
    "- `function_name` (string): \n",
    "    - name of Cognite Function to deploy (i.e., folder with name `cf_*function_name*`)\n",
    "- `calculation_function` (string): \n",
    "    - name of main calculation function to run, should be defined in transformation.py (in the folder `cf_*function_name*`) as `main_*calculation_function*`\n",
    "- `schedule_name` (string):\n",
    "    - name of schedule to set up for the Cognite Function. NB: make sure name is unique to avoid overwriting already existing schedules for a particular Cognite Function! If setting up multiple schedules for the same Cognite Function, one for each input time series, a good advice to keep them organized is to use the name of the time series as the name of the schedules \n",
    "- `aggregate` (dictionary):\n",
    "    - information about any aggregations to perform in the calculation.\n",
    "    - if **not** performing any aggregates, leave the dictionary empty!\n",
    "    - if performing aggregates, two keys must be specified:\n",
    "    1. `period` (string):\n",
    "        - the time range defining the aggregated period\n",
    "        - valid values: `[\"minute\", \"hour\", \"day\", \"month\", \"year\"]`\n",
    "    2. `type` (string):\n",
    "        - what type of aggregate to perform\n",
    "        - valid values: any aggregation supported by `pandas`, e.g., `\"mean\"`, `\"max\"`, ... \n",
    "- `sampling_rate` (int): \n",
    "    - sampling rate of input time series, given in seconds\n",
    "- `cron_interval_min` (string): \n",
    "    - minute-interval to run schedule at (NB: currently only supported for min-interval [1, 60)). The number should be provided as string.\n",
    "- `backfill_period` (int): \n",
    "    - the period (default: number of days) back in time to perform backfilling\n",
    "    - if performing aggregates, it is the number of aggregated periods (e.g., if aggregating over month, a value of 3 will backfill three month back in time)\n",
    "- `backfill_hour` (int):\n",
    "    - the hour of the day to perform backfilling\n",
    "- `backfill_min_start` (int):\n",
    "    - performs backfilling for any scheduled call that falls within hour=`backfill_hour` and minute=`[backfill_min_start, backfill_min_start+cron_interval_min]`\n",
    "- `testing` (bool):\n",
    "    - defaults to `False`. Set to `True` if running unit tests\n",
    "- `add_packages` (list): \n",
    "    - additional packages required to run the calculations in `transformations.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_input_names = [\"VAL_17-FI-9101-286:VALUE\", \"VAL_17-PI-95709-258:VALUE\", \"VAL_11-PT-92363B:X.Value\", \"VAL_11-XT-95067B:Z.X.Value\"] # Inputs to IdealPowerConsumption function # [\"VAL_11-XT-95067B:Z.X.Value\", 87.8, \"CF_IdealPowerConsumption\"] # Inputs to WasterEnergy function\n",
    "ts_input_names = [\"VAL_11-LT-95107A:X.Value\"]\n",
    "# ts_output_names = [\"CF_IdealPowerConsumption\"]\n",
    "ts_output = {\"names\": [\"hourly_avg_drainage_description_unit\"],\n",
    "             \"description\": [\"Hourly average drainage from pump\"], #[\"Daily average drainage from pump\"]\n",
    "             \"unit\": [\"m3/min\"]} #[\"m3/min\"]\n",
    "\n",
    "function_name = \"hourly-avg-drainage\"\n",
    "calculation_function = \"aggregate\"\n",
    "schedule_name = ts_input_names[0]\n",
    "\n",
    "aggregate = {}\n",
    "aggregate[\"period\"] = \"hour\"\n",
    "aggregate[\"type\"] = \"mean\"\n",
    "\n",
    "sampling_rate = 60 #\n",
    "cron_interval_min = str(15) #\n",
    "assert int(cron_interval_min) < 60 and int(cron_interval_min) >= 1\n",
    "backfill_period = 3\n",
    "backfill_hour = 15 # 23\n",
    "backfill_min_start = 30\n",
    "\n",
    "add_packages = [\"statsmodels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Optional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tank_volume = 240\n",
    "derivative_value_excl = 0.002\n",
    "lowess_frac = 0.001\n",
    "lowess_delta = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert parameters into data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "backfill_min_start = min(59, backfill_min_start)\n",
    "\n",
    "data_dict = {'ts_input_names':ts_input_names,\n",
    "            'ts_output':ts_output,\n",
    "            'function_name': f\"cf_{function_name}\",\n",
    "            'schedule_name': schedule_name,\n",
    "            'calculation_function': f\"main_{calculation_function}\",\n",
    "            'granularity': sampling_rate,\n",
    "            'dataset_id': 1832663593546318, # Center of Excellence - Analytics dataset\n",
    "            'cron_interval_min': cron_interval_min,\n",
    "            'aggregate': aggregate,\n",
    "            'testing': False,\n",
    "            'backfill_period': backfill_period, # days by default (if not doing aggregates)\n",
    "            'backfill_hour': backfill_hour, # 23: backfilling to be scheduled at last hour of day as default\n",
    "            'backfill_min_start': backfill_min_start, 'backfill_min_end': min(59.9, backfill_min_start + int(cron_interval_min)),\n",
    "            'calc_params': {\n",
    "                'derivative_value_excl':derivative_value_excl, 'tank_volume':tank_volume,\n",
    "                'lowess_frac': lowess_frac, 'lowess_delta': lowess_delta, #'aggregate_period': aggregate[\"period\"]\n",
    "            }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Instantiate Cognite Function ---\n",
    "\n",
    "Set up folder structure for the Cognite Function as required by the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing __init__.py ...\n",
      "Writing handler.py ...\n",
      "Writing transformation.py ...\n",
      "Created requirements.txt in c:/Users/vetnev/OneDrive - Aker BP/Documents/First Task/opshub-task1/src/cf_power-Template\n",
      "Packages to add:  ['numpy', 'ipykernel', 'cognite-sdk', 'python-dotenv', 'pytest', 'pandas']\n",
      "\n",
      "Using version ^1.26.3 for numpy\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 1 install, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing numpy (1.26.3)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^6.28.0 for ipykernel\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 28 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing six (1.16.0)\n",
      "  â€¢ Installing asttokens (2.4.1)\n",
      "  â€¢ Installing executing (2.0.1)\n",
      "  â€¢ Installing parso (0.8.3)\n",
      "  â€¢ Installing platformdirs (4.1.0)\n",
      "  â€¢ Installing pure-eval (0.2.2)\n",
      "  â€¢ Installing pywin32 (306)\n",
      "  â€¢ Installing traitlets (5.14.1)\n",
      "  â€¢ Installing wcwidth (0.2.13)\n",
      "  â€¢ Installing colorama (0.4.6)\n",
      "  â€¢ Installing decorator (5.1.1)\n",
      "  â€¢ Installing jedi (0.19.1)\n",
      "  â€¢ Installing jupyter-core (5.7.1)\n",
      "  â€¢ Installing matplotlib-inline (0.1.6)\n",
      "  â€¢ Installing prompt-toolkit (3.0.43)\n",
      "  â€¢ Installing pygments (2.17.2)\n",
      "  â€¢ Installing python-dateutil (2.8.2)\n",
      "  â€¢ Installing pyzmq (25.1.2)\n",
      "  â€¢ Installing stack-data (0.6.3)\n",
      "  â€¢ Installing tornado (6.4)\n",
      "  â€¢ Installing comm (0.2.1)\n",
      "  â€¢ Installing debugpy (1.8.0)\n",
      "  â€¢ Installing ipython (8.20.0)\n",
      "  â€¢ Installing jupyter-client (8.6.0)\n",
      "  â€¢ Installing nest-asyncio (1.5.8)\n",
      "  â€¢ Installing packaging (23.2)\n",
      "  â€¢ Installing psutil (5.9.7)\n",
      "  â€¢ Installing ipykernel (6.28.0)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^7.13.4 for cognite-sdk\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 16 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing pycparser (2.21)\n",
      "  â€¢ Installing cffi (1.16.0)\n",
      "  â€¢ Installing certifi (2023.11.17)\n",
      "  â€¢ Installing charset-normalizer (3.3.2)\n",
      "  â€¢ Installing cryptography (41.0.7)\n",
      "  â€¢ Installing idna (3.6)\n",
      "  â€¢ Installing urllib3 (2.1.0)\n",
      "  â€¢ Installing oauthlib (3.2.2)\n",
      "  â€¢ Installing pyjwt (2.8.0)\n",
      "  â€¢ Installing requests (2.31.0)\n",
      "  â€¢ Installing msal (1.26.0)\n",
      "  â€¢ Installing protobuf (4.25.2)\n",
      "  â€¢ Installing requests-oauthlib (1.3.1)\n",
      "  â€¢ Installing typing-extensions (4.9.0)\n",
      "  â€¢ Installing tzdata (2023.4)\n",
      "  â€¢ Installing cognite-sdk (7.13.4)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^1.0.0 for python-dotenv\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 1 install, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing python-dotenv (1.0.0)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^7.4.4 for pytest\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 3 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing iniconfig (2.0.0)\n",
      "  â€¢ Installing pluggy (1.3.0)\n",
      "  â€¢ Installing pytest (7.4.4)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^2.1.4 for pandas\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 2 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing pytz (2023.3.post1)\n",
      "  â€¢ Installing pandas (2.1.4)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Installing dependencies from lock file\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "Created requirements.txt in c:/Users/vetnev/OneDrive - Aker BP/Documents/First Task/opshub-task1/src/cf_power-Template\n"
     ]
    }
   ],
   "source": [
    "generate_cf(function_name, add_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Define transformation function ---\n",
    "\n",
    "In this step, modify `transformation.py` to include your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Deploy Cognite Function in one go ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single call\n",
    "\n",
    "Initial transformation is data-intensive. A scheduled call will likely time out. Instead, do a separate call first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cognite Function created. Waiting for deployment status to be ready ...\n",
      "Ready for deployement.\n",
      "Calling Cognite Function individually ...\n",
      "... Done\n"
     ]
    }
   ],
   "source": [
    "deploy_cognite_functions(data_dict, client,\n",
    "                         single_call=True, scheduled_call=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduled call\n",
    "\n",
    "For subsequent calls, transformations are only done on current date, not too data intensive. This can be handled by scheduled calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing schedule to start sharp at next minute ...\n",
      "Setting up Cognite Function schedule at time 2024-01-16 12:29:00+00:00 ...\n",
      "... Done\n"
     ]
    }
   ],
   "source": [
    "deploy_cognite_functions(data_dict, client,\n",
    "                         single_call=False, scheduled_call=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firsttask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
