{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for deploying Cognite Function\n",
    "\n",
    "Run all cells sequentially until `Experimental` section to deploy your Cognite Function.\n",
    "\n",
    "Modifications are done in `Inputs` section, where you need to supply relevant input parameters as required by instantiation, calculations and deployment of your Cognite Function. The input parameters related to calculations and deployment are stored in `data_dict`. There are two types of input parameters:\n",
    "- A: General parameters required for deployment of any Cognite Function\n",
    "- B: Optional (calculation-specific) parameters used as input to your calculation function. These should enter `data_dict[\"calc_params\"]` as key-value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Authentication ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from cognite.client.data_classes import functions\n",
    "from cognite.client.data_classes.functions import FunctionSchedulesList\n",
    "from cognite.client.data_classes.functions import FunctionSchedule\n",
    "\n",
    "from initialize import initialize_client\n",
    "from deploy_cognite_functions import deploy_cognite_functions\n",
    "from generate_cf import generate_cf\n",
    "\n",
    "cdf_env = \"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set limit on function calls - don't think it's really necessary ...\n",
    "func_limits = functions.FunctionsLimits(timeout_minutes=60, cpu_cores=0.25, memory_gb=1, runtimes=[\"py39\"], response_size_mb=2)\n",
    "client = initialize_client(cdf_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.time_series.delete(external_id=\"VAL_17-FI-9101-286:VALUE.COPY\")\n",
    "# client.time_series.delete(external_id=\"test_CF\")\n",
    "# client.time_series.delete(external_id=\"TemplateVsCharts_Charts_1702381784041_TS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Inputs ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Required parameters\n",
    "- `ts_input_names`: \n",
    "    - names of input time series (given as list, even if only one input). Must be given in same order as calculations are performed in `transformations.py`\n",
    "- `ts_output_names`: \n",
    "    - names of output time series (also given as list). NB: if multiple time series outputs, order of ts_output_names must correspond to order in ts_input_names.\n",
    "- `function_name`: \n",
    "    - name of Cognite Function to deploy (i.e., folder with name `cf_*function_name*`)\n",
    "- `calculation_function`: \n",
    "    - name of main calculation function to run, should be defined in transformation.py (in the folder `cf_*function_name*`) as `main_*calculation_function*`\n",
    "- `sampling_rate`: \n",
    "    - sampling rate of input time series, given in seconds\n",
    "- `cron_interval_min`: \n",
    "    - minute-interval to run schedule at (NB: currently only supported for min-interval [1, 60))\n",
    "- `backfill_days`: \n",
    "    - number of days back in time to perform backfilling\n",
    "- `add_packages`: \n",
    "    - list of additional packages required to run the calculations in `transformations.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = False\n",
    "\n",
    "# ts_input_names = [\"VAL_17-FI-9101-286:VALUE\", \"VAL_17-PI-95709-258:VALUE\", \"VAL_11-PT-92363B:X.Value\", \"VAL_11-XT-95067B:Z.X.Value\"] # Inputs to IdealPowerConsumption function\n",
    "# ts_input_names = [\"VAL_11-XT-95067B:Z.X.Value\", 87.8, \"TEST_IdealPowerConsumption\"] # Inputs to WasterEnergy function\n",
    "ts_input_names = [\"VAL_11-PT-92363B:X.Value\"]\n",
    "# ts_output_names = [\"VAL_17-FI-9101-286:MULTIPLE.Test\", \"VAL_17-PI-95709-258:MULTIPLE.Test\", \"VAL_11-PT-92363B:MULTIPLE.Test\"]#, \"VAL_11-XT-95067B:MULTIPLE.Test\"]\n",
    "# ts_output_names = [\"TEST_IdealPowerConsumption\"]\n",
    "# ts_output_names = [\"TEST_WastedEnergy\"]\n",
    "ts_output_names = [\"TemplateVsCharts_Template\"]\n",
    "\n",
    "function_name = \"templatevscharts-template\"\n",
    "calculation_function = \"exp\"\n",
    "start_time = None # setting to None gives default start_time defined by schedule interval\n",
    "\n",
    "if demo:\n",
    "    # client.time_series.delete(external_id=\"VAL_11-LT-95034A:X.CDF.D.AVG.LeakValue\")\n",
    "    ts_input_names = [\"VAL_11-LT-95034A:X.Value\"] # VOLUME PERCENTAGE\n",
    "    ts_output_names = [\"VAL_11-LT-95034A:X.CDF.D.AVG.LeakValue\"] # DAILY AVERAGE DRAINAGE\n",
    "    function_name = \"daily-avg-drainage\"\n",
    "    calculation_function = \"daily_avg_drainage\"\n",
    "    start_time = pd.to_datetime(pd.Timestamp.now().date()) # need start date to be start (midnight) of current date\n",
    "\n",
    "sampling_rate = 60 #\n",
    "cron_interval_min = str(15) #\n",
    "assert int(cron_interval_min) < 60 and int(cron_interval_min) >= 1\n",
    "backfill_days = 3\n",
    "backfill_hour = 14 # 23\n",
    "backfill_min_start = 30\n",
    "\n",
    "add_packages = [\"indsl\"] # REQUIRED FOR CALCULATIONS DONE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Optional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tank_volume = 1400\n",
    "derivative_value_excl = 0.002\n",
    "lowess_frac = 0.001\n",
    "lowess_delta = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert parameters into data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "backfill_min_start = min(59, backfill_min_start)\n",
    "backfill_min_end = min(59.9, backfill_min_start + int(cron_interval_min))\n",
    "\n",
    "data_dict = {'ts_input_names':ts_input_names,\n",
    "            'ts_output_names':ts_output_names,\n",
    "            'function_name': f\"cf_{function_name}\",\n",
    "            'calculation_function': f\"main_{calculation_function}\",\n",
    "            'granularity': sampling_rate,\n",
    "            'dataset_id': 1832663593546318, # Center of Excellence - Analytics dataset\n",
    "            'backfill_days': backfill_days,\n",
    "            'backfill_hour': backfill_hour, # 23: backfilling to be scheduled at last hour of day as default\n",
    "            'backfill_min_start': backfill_min_start, 'backfill_min_end': backfill_min_end,\n",
    "            'calc_params': {\n",
    "                'derivative_value_excl':derivative_value_excl, 'tank_volume':tank_volume,\n",
    "                'lowess_frac': lowess_frac, 'lowess_delta': lowess_delta, 'time_unit': \"1m\"\n",
    "            }}\n",
    "\n",
    "if start_time is not None:\n",
    "    data_dict['start_time'] = start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Instantiate Cognite Function ---\n",
    "\n",
    "Set up folder structure for the Cognite Function as required by the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing __init__.py ...\n",
      "Writing handler.py ...\n",
      "Writing transformation.py ...\n",
      "Created requirements.txt in c:/Users/vetnev/OneDrive - Aker BP/Documents/First Task/opshub-task1/src/cf_templatevscharts-template\n",
      "Packages to add:  ['pandas', 'numpy', 'python-dotenv', 'cognite-sdk', 'ipykernel', 'pytest', 'indsl']\n",
      "\n",
      "Using version ^2.1.4 for pandas\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 6 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing six (1.16.0)\n",
      "  â€¢ Installing numpy (1.26.2)\n",
      "  â€¢ Installing python-dateutil (2.8.2)\n",
      "  â€¢ Installing pytz (2023.3.post1)\n",
      "  â€¢ Installing tzdata (2023.3)\n",
      "  â€¢ Installing pandas (2.1.4)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^1.26.2 for numpy\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^1.0.0 for python-dotenv\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 1 install, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing python-dotenv (1.0.0)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^7.6.0 for cognite-sdk\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 16 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing pycparser (2.21)\n",
      "  â€¢ Installing cffi (1.16.0)\n",
      "  â€¢ Installing certifi (2023.11.17)\n",
      "  â€¢ Installing charset-normalizer (3.3.2)\n",
      "  â€¢ Installing cryptography (41.0.7)\n",
      "  â€¢ Installing idna (3.6)\n",
      "  â€¢ Installing urllib3 (2.1.0)\n",
      "  â€¢ Installing oauthlib (3.2.2)\n",
      "  â€¢ Installing pyjwt (2.8.0)\n",
      "  â€¢ Installing requests (2.31.0)\n",
      "  â€¢ Installing msal (1.26.0)\n",
      "  â€¢ Installing protobuf (4.25.1)\n",
      "  â€¢ Installing requests-oauthlib (1.3.1)\n",
      "  â€¢ Installing sortedcontainers (2.4.0)\n",
      "  â€¢ Installing typing-extensions (4.9.0)\n",
      "  â€¢ Installing cognite-sdk (7.6.0)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^6.27.1 for ipykernel\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 26 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing asttokens (2.4.1)\n",
      "  â€¢ Installing executing (2.0.1)\n",
      "  â€¢ Installing parso (0.8.3)\n",
      "  â€¢ Installing platformdirs (4.1.0)\n",
      "  â€¢ Installing pure-eval (0.2.2)\n",
      "  â€¢ Installing pywin32 (306)\n",
      "  â€¢ Installing traitlets (5.14.0)\n",
      "  â€¢ Installing wcwidth (0.2.12)\n",
      "  â€¢ Installing colorama (0.4.6)\n",
      "  â€¢ Installing decorator (5.1.1)\n",
      "  â€¢ Installing jedi (0.19.1)\n",
      "  â€¢ Installing jupyter-core (5.5.0)\n",
      "  â€¢ Installing matplotlib-inline (0.1.6)\n",
      "  â€¢ Installing prompt-toolkit (3.0.43)\n",
      "  â€¢ Installing pygments (2.17.2)\n",
      "  â€¢ Installing pyzmq (25.1.2)\n",
      "  â€¢ Installing stack-data (0.6.3)\n",
      "  â€¢ Installing tornado (6.4)\n",
      "  â€¢ Installing comm (0.2.0)\n",
      "  â€¢ Installing debugpy (1.8.0)\n",
      "  â€¢ Installing ipython (8.18.1)\n",
      "  â€¢ Installing jupyter-client (8.6.0)\n",
      "  â€¢ Installing nest-asyncio (1.5.8)\n",
      "  â€¢ Installing packaging (23.2)\n",
      "  â€¢ Installing psutil (5.9.6)\n",
      "  â€¢ Installing ipykernel (6.27.1)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Using version ^7.4.3 for pytest\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 3 installs, 0 updates, 0 removals\n",
      "\n",
      "  â€¢ Installing iniconfig (2.0.0)\n",
      "  â€¢ Installing pluggy (1.3.0)\n",
      "  â€¢ Installing pytest (7.4.3)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Error running Poetry command:\n",
      "\n",
      "Because indsl (8.2.1) depends on pandas (>=2.0.3,<2.1.0)\n",
      " and no versions of indsl match >8.2.1,<9.0.0, indsl (>=8.2.1,<9.0.0) requires pandas (>=2.0.3,<2.1.0).\n",
      "So, because cf-templatevscharts-template depends on both pandas (^2.1.4) and indsl (^8.2.1), version solving failed.\n",
      "\n",
      "Installing dependencies from lock file\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "Created requirements.txt in c:/Users/vetnev/OneDrive - Aker BP/Documents/First Task/opshub-task1/src/cf_templatevscharts-template\n"
     ]
    }
   ],
   "source": [
    "generate_cf(function_name, add_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Define transformation function ---\n",
    "\n",
    "In this step, modify `transformation.py` to include your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Deploy Cognite Function in one go ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single call\n",
    "\n",
    "Initial transformation is data-intensive. A scheduled call will likely time out. Instead, do a separate call first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cognite Function created. Waiting for deployment status to be ready ...\n",
      "Ready for deployement.\n",
      "Calling Cognite Function individually ...\n",
      "... Done\n"
     ]
    }
   ],
   "source": [
    "deploy_cognite_functions(data_dict, client, cron_interval_min,\n",
    "                         single_call=True, scheduled_call=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduled call\n",
    "\n",
    "For subsequent calls, transformations are only done on current date, not too data intensive. This can be handled by scheduled calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Cognite Function schedule ...\n",
      "... Done\n"
     ]
    }
   ],
   "source": [
    "deploy_cognite_functions(data_dict, client, cron_interval_min,\n",
    "                         single_call=False, scheduled_call=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Experimental ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=3600)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client.time_series.delete(external_id=\"test_CF_create_timeseries\")\n",
    "client.time_series.list(name=\"VAL_17-FI-9101-286:VALUE\").to_pandas()[\"asset_id\"][0]\n",
    "from datetime import timedelta\n",
    "timedelta(days=1/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8236094801741723"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# data[col]: prints pd.Series object\n",
    "# data[[col]]: prints pd.DataFrame object\n",
    "\n",
    "myfunc = client.functions.retrieve(external_id=\"cf_wasted-energy\")\n",
    "my_schedule_id = client.functions.schedules.list(\n",
    "                name=\"cf_wasted-energy\").to_pandas().id[0]\n",
    "myfunc.list_calls(schedule_id=my_schedule_id)\n",
    "test = client.functions.calls.retrieve(call_id=3005253751851002, function_id=84587311037983).get_response()\n",
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': {'0': 1, '1': 2, '2': 3, '3': 4, '4': 5}, 'gsgg': 'null'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "test = pd.DataFrame([[1,2,3,4,5], [5,6,7,6,5]]).T\n",
    "ast.literal_eval('{\"test\": None}')\n",
    "orig = ast.literal_eval(test[0].to_json())\n",
    "ast.literal_eval(json.dumps({\"test\": orig, \"gsgg\": json.dumps(None)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sid = client.functions.schedules.list(function_id=func_drainage.id).to_pandas().id[0]\n",
    "scid = func_drainage.list_calls(schedule_id=sid, limit=-1).to_pandas()\n",
    "resp = func_drainage.retrieve_call(id=scid).get_response()\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_func = client.functions.retrieve(external_id=data_dict[\"function_name\"])\n",
    "my_schedule_id = client.functions.schedules.list(\n",
    "            name=data_dict[\"function_name\"]).to_pandas().id[0]\n",
    "all_calls = my_func.list_calls(\n",
    "            schedule_id=my_schedule_id, limit=-1).to_pandas()\n",
    "all_calls.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.date_range(start=datetime(2023,11,16,0,0), end=datetime(2023,11,16,3,51), freq=\"T\")\n",
    "extid = client.time_series.list(name=\"VAL_17-FI-9101-286:VALUE\")[0].external_id\n",
    "ts_orig_all = client.time_series.data.retrieve(external_id=extid,\n",
    "                                                   limit=20,\n",
    "                                                   ).to_pandas()\n",
    "ts_orig_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalizing Cognite Functions - sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ts_all = {\n",
    "        'ts_A': {'granularity':15, 'var':'a'},\n",
    "        'ts_B': {'granularity':10, 'b_specific':[1,2,3]},\n",
    "        'ts_X': {'max_days':8, 'thermo_coeff': 0.05, 'filter':'lowess'},\n",
    "        'ts_Y': {'tot_days': 40},\n",
    "        'out': 'test',\n",
    "        'in': {'granularity':15, 'b_specific':[1,2,3,4]},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "func_drainage = client.functions.retrieve(external_id=\"draiange\")\n",
    "func_thermo = client.functions.retrieve(external_id=\"thermo\")\n",
    "\n",
    "func_drainage_schedule = []\n",
    "func_thermo_schedule = []\n",
    "\n",
    "\"\"\"Create individual schedules for three time series running drainage-Cognite-function\"\"\"\n",
    "for ts in ['A', 'B', 'Y']:\n",
    "    func_schedule = client.functions.schedules.create(\n",
    "        name=f\"avg-leak-{ts}\",\n",
    "        cron_expression=f\"*/{cron_interval_min} * * * *\",\n",
    "        function_id=func_drainage.id, # SAME function id\n",
    "        description=f\"Leak rate calculation for time series {ts}\",\n",
    "        data=ts_all[f'ts_{ts}'] # DIFFERENT data dictionaries\n",
    "    )\n",
    "    func_drainage_schedule.append(func_schedule)\n",
    "\n",
    "func_drainage_X = client.functions.schedules.create(\n",
    "    name=f\"avg-leak-X\",\n",
    "    cron_expression=f\"*/{cron_interval_min} * * * *\",\n",
    "    function_id=func_drainage.id,\n",
    "    description=f\"Leak rate calculation for time series X\",\n",
    "    data=ts_all['ts_X'],\n",
    ")\n",
    "\n",
    "\"\"\"Run schedules for DIFFERENT Cognite Functions on SAME time series Y.\n",
    "ALTERNATIVE 1: Each one with SAME data dictionary\"\"\"\n",
    "for func in [func_drainage, func_thermo]:\n",
    "    func_schedule = client.functions.schedules.create(\n",
    "        name=f\"tsY_{func.name}\",\n",
    "        cron_expression=f\"*/{cron_interval_min} * * * *\",\n",
    "        function_id=func.id, # DIFFERENT function ids\n",
    "        description=f\"{func.name} calculation for time series Y\",\n",
    "        data=ts_all['ts_Y'] # SAME data dictionary\n",
    "    )\n",
    "    func_thermo_schedule.append(func_schedule)\n",
    "\n",
    "\"\"\"ALTERNATIVE 2: Each one with DIFFERENT data dictionaries\"\"\"\n",
    "for func in [func_drainage, func_thermo]:\n",
    "    func_schedule = client.functions.schedules.create(\n",
    "        name=f\"tsY_{func.name}\",\n",
    "        cron_expression=f\"*/{cron_interval_min} * * * *\",\n",
    "        function_id=func.id, # DIFFERENT function ids\n",
    "        description=f\"{func.name} calculation for time series Y\",\n",
    "        data=ts_all[f'ts_Y'][func.name] # DIFFERENT data dictionaries\n",
    "    )\n",
    "    func_thermo_schedule.append(func_schedule)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firsttask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
